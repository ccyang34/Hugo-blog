#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIè¡Œä¸šèµ„é‡‘æµå‘AIåˆ†æå¸ˆ
æ•´åˆæ–°æµªè´¢ç»è¡Œä¸šæ•°æ®è·å–ã€æ™ºèƒ½åˆ†æå’ŒAIæ·±åº¦è§£è¯»åŠŸèƒ½
ç»“åˆå¤šæ—¶é—´ç»´åº¦èµ„é‡‘æµå‘åˆ†æ

Author: AI Assistant
Date: 2024-12-02
"""

# =============================================================================
# å¯¼å…¥æ¨¡å—
# =============================================================================

# æ ‡å‡†åº“å¯¼å…¥
import os
import time
import warnings
import json
from datetime import datetime
from io import StringIO

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
import requests
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import pytz


# =============================================================================
# é…ç½®å¸¸é‡
# =============================================================================

# ---------------- DeepSeek API é…ç½® ----------------
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "sk-063857d175bd48038684520e7b6ec934")
DEEPSEEK_BASE_URL = "https://api.deepseek.com"
MODEL_NAME = "deepseek-chat"

# Hugo åšå®¢é…ç½®
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
HUGO_BLOG_DIR = os.path.dirname(SCRIPT_DIR)
HUGO_CONTENT_DIR = os.path.join(HUGO_BLOG_DIR, "content", "posts")
HUGO_IMAGES_DIR = os.path.join(HUGO_BLOG_DIR, "static", "images", "charts")


# ---------------- æ—¶åŒºé…ç½® ----------------
BEIJING_TZ = pytz.timezone('Asia/Shanghai')

# =============================================================================
# å·¥å…·å‡½æ•°
# =============================================================================

def get_beijing_time():
    """
    è·å–åŒ—äº¬æ—¶é—´
    
    Returns:
        datetime: åŒ—äº¬æ—¶é—´å¯¹è±¡
    """
    return datetime.now(BEIJING_TZ)

# =============================================================================
# æ•°æ®è·å–æ¨¡å—
# =============================================================================

class DataFetcher:
    """
    æ•°æ®è·å–ç±» - ä¸“æ³¨äºè¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘æ•°æ®è·å–
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®è·å–å™¨"""
        self.base_url = "https://vip.stock.finance.sina.com.cn/quotes_service/api/json_v2.php/MoneyFlow.ssl_bkzjlxt"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36 Edg/142.0.0.0',
            'Accept': 'application/json, text/javascript, */*; q=0.01',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
            'Referer': 'https://vip.stock.finance.sina.com.cn/moneyflow/',
            'X-Requested-With': 'XMLHttpRequest'
        }
        
        # åˆ›å»ºsessionä»¥é¿å…ä»£ç†é—®é¢˜
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.session.trust_env = False
    
    def fetch_industry_flow_data(self, page=1, num=20, sort='cate_id', asc=1, fenlei=2):
        """
        è·å–è¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘æ•°æ®
        
        Args:
            page: é¡µç ï¼Œé»˜è®¤1
            num: æ¯é¡µæ•°é‡ï¼Œé»˜è®¤20
            sort: æ’åºå­—æ®µï¼Œé»˜è®¤cate_idï¼ˆè¡Œä¸šä»£ç ï¼‰
            asc: æ’åºæ–¹å‘ï¼Œ0é™åºï¼Œ1å‡åºï¼Œé»˜è®¤1
            fenlei: åˆ†ç±»ï¼Œ2è¡¨ç¤ºè·å–è¡Œä¸šèµ„é‡‘æµå‘æ•°æ®
            
        Returns:
            dict: è¿”å›çš„è¡Œä¸šJSONæ•°æ®ï¼Œå¤±è´¥è¿”å›None
        """
        url = self.base_url
        params = {
            'page': page,
            'num': num,
            'sort': sort,
            'asc': asc,
            'fenlei': fenlei
        }
        
        try:
            print(f"æ­£åœ¨è·å–ç¬¬{page}é¡µè¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘æ•°æ®...")
            response = self.session.get(url, params=params, timeout=15)
            response.raise_for_status()
            response.encoding = 'gbk'  # æ–°æµªè´¢ç»ä½¿ç”¨GBKç¼–ç 
            
            # è§£æJSONæ•°æ®
            content = response.text.strip()
            if content.startswith('[') and content.endswith(']'):
                data = json.loads(content)
            else:
                # å¤„ç†å¯èƒ½çš„JSONPæ ¼å¼
                start = content.find('(') + 1
                end = content.rfind(')')
                if start > 0 and end > start:
                    data = json.loads(content[start:end])
                else:
                    data = json.loads(content)
            
            print(f"ç¬¬{page}é¡µè·å–æˆåŠŸï¼Œå…±{len(data)}æ¡æ•°æ®")
            return data
            
        except requests.exceptions.RequestException as e:
            print(f"è·å–ç¬¬{page}é¡µæ•°æ®å‡ºé”™: {str(e)}")
            return None
        except json.JSONDecodeError as e:
            print(f"è§£æç¬¬{page}é¡µJSONæ•°æ®å‡ºé”™: {str(e)}")
            return None
        except Exception as e:
            print(f"ç¬¬{page}é¡µæ•°æ®å¤„ç†å‡ºé”™: {str(e)}")
            return None
    
    def parse_industry_flow_data(self, raw_data):
        """
        è§£æè¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘æ•°æ®
        
        Args:
            raw_data: åŸå§‹è¡Œä¸šJSONæ•°æ®
            
        Returns:
            list: è§£æåçš„è¡Œä¸šæ•°æ®åˆ—è¡¨
        """
        if not raw_data:
            return []
        
        parsed_data = []
        for item in raw_data:
            try:
                # è§£æè¡Œä¸šæ•°æ®å­—æ®µï¼Œé€‚é…å¤šæ—¶é—´ç»´åº¦
                parsed_item = {
                    'è¡Œä¸šä»£ç ': item.get('category', ''),
                    'è¡Œä¸šåç§°': item.get('name', ''),
                    '3æ—¥å‡€æµå…¥': float(item.get('netamount_3', 0)),
                    '3æ—¥å‡€æµå…¥å æ¯”': float(item.get('ratioamount_3', 0)),
                    '3æ—¥å¹³å‡æ¶¨è·Œå¹…': float(item.get('avg_changeratio_3', 0)),
                    '3æ—¥æµå…¥æµå‡ºæ¯”': float(item.get('r0x_ratio_3', 0)),
                    '5æ—¥å‡€æµå…¥': float(item.get('netamount_5', 0)),
                    '5æ—¥å‡€æµå…¥å æ¯”': float(item.get('ratioamount_5', 0)),
                    '5æ—¥å¹³å‡æ¶¨è·Œå¹…': float(item.get('avg_changeratio_5', 0)),
                    '5æ—¥æµå…¥æµå‡ºæ¯”': float(item.get('r0x_ratio_5', 0)),
                    '10æ—¥å‡€æµå…¥': float(item.get('netamount_10', 0)),
                    '10æ—¥å‡€æµå…¥å æ¯”': float(item.get('ratioamount_10', 0)),
                    '10æ—¥å¹³å‡æ¶¨è·Œå¹…': float(item.get('avg_changeratio_10', 0)),
                    '10æ—¥æµå…¥æµå‡ºæ¯”': float(item.get('r0x_ratio_10', 0)),
                    'æ•°æ®æ›´æ–°æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                parsed_data.append(parsed_item)
                
            except (ValueError, TypeError) as e:
                print(f"è§£ææ•°æ®é¡¹æ—¶å‡ºé”™: {str(e)}, æ•°æ®: {item}")
                continue
        
        return parsed_data
    
    def collect_batch_data(self, total_pages=8, page_size=20):
        """
        æ‰¹é‡è·å–å¤šé¡µè¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘æ•°æ®
        
        Args:
            total_pages: æ€»é¡µæ•°ï¼Œé»˜è®¤8é¡µï¼ˆå®Œæ•´æ•°æ®ï¼‰
            page_size: æ¯é¡µå¤§å°ï¼Œé»˜è®¤20æ¡
            
        Returns:
            list: åˆå¹¶åçš„æ‰€æœ‰è¡Œä¸šæ•°æ®
        """
        all_data = []
        
        print(f"=== å¼€å§‹æ‰¹é‡è·å–è¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘æ•°æ® ===")
        print(f"ç›®æ ‡: è·å–{total_pages}é¡µè¡Œä¸šæ•°æ®ï¼Œæ¯é¡µ{page_size}æ¡")
        
        for page in range(1, total_pages + 1):
            print(f"\n--- è·å–ç¬¬{page}é¡µæ•°æ® ---")
            
            # è·å–åŸå§‹æ•°æ®
            raw_data = self.fetch_industry_flow_data(page=page, num=page_size)
            if raw_data:
                # è§£ææ•°æ®
                parsed_data = self.parse_industry_flow_data(raw_data)
                if parsed_data:
                    all_data.extend(parsed_data)
                    print(f"ç¬¬{page}é¡µè§£ææˆåŠŸï¼Œè·å¾—{len(parsed_data)}æ¡æ•°æ®")
                else:
                    print(f"ç¬¬{page}é¡µæ•°æ®è§£æå¤±è´¥")
            else:
                print(f"ç¬¬{page}é¡µæ•°æ®è·å–å¤±è´¥")
            
            # æ·»åŠ é—´éš”ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            if page < total_pages:
                print("ç­‰å¾…1ç§’...")
                time.sleep(1)
        
        print(f"\n=== æ‰¹é‡è·å–å®Œæˆ ===")
        print(f"æ€»è®¡è·å¾— {len(all_data)} æ¡æ•°æ®")
        
        return all_data

# =============================================================================
# æ•°æ®åˆ†ææ¨¡å—
# =============================================================================

class DataAnalyzer:
    """
    æ•°æ®åˆ†æç±» - ä¸“æ³¨äºè¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘æ•°æ®åˆ†æ
    """
    
    @staticmethod
    def save_to_csv(data, filename=None):
        """
        ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶
        
        Args:
            data: è¦ä¿å­˜çš„æ•°æ®
            filename: æ–‡ä»¶åï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            str: ä¿å­˜çš„æ–‡ä»¶åï¼Œå¤±è´¥è¿”å›None
        """
        if not data:
            print("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return
        
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"AIè¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘æ•°æ®_{timestamp}.csv"
        
        try:
            df = pd.DataFrame(data)
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            print(f"æ•°æ®å·²ä¿å­˜åˆ° {filename}")
            return filename
        except Exception as e:
            print(f"ä¿å­˜CSVæ–‡ä»¶å‡ºé”™: {str(e)}")
            return None
    
    @staticmethod
    def get_industry_summary(data):
        """
        è·å–è¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘æ¦‚å†µç»Ÿè®¡
        
        Args:
            data: è¡Œä¸šæ•°æ®åˆ—è¡¨
            
        Returns:
            dict: æ±‡æ€»ç»Ÿè®¡æ•°æ®
        """
        if not data:
            return {}
        
        df = pd.DataFrame(data)
        
        # åŸºç¡€ç»Ÿè®¡
        total_industries = len(df)
        positive_3d = len(df[df['3æ—¥å¹³å‡æ¶¨è·Œå¹…'] > 0])
        positive_5d = len(df[df['5æ—¥å¹³å‡æ¶¨è·Œå¹…'] > 0])
        positive_10d = len(df[df['10æ—¥å¹³å‡æ¶¨è·Œå¹…'] > 0])
        
        # å‡€æµå…¥ç»Ÿè®¡
        total_inflow_3d = df['3æ—¥å‡€æµå…¥'].sum()
        total_inflow_5d = df['5æ—¥å‡€æµå…¥'].sum()
        total_inflow_10d = df['10æ—¥å‡€æµå…¥'].sum()
        
        # æµå…¥æµå‡ºæ¯”ç»Ÿè®¡
        avg_ratio_3d = df['3æ—¥æµå…¥æµå‡ºæ¯”'].mean()
        avg_ratio_5d = df['5æ—¥æµå…¥æµå‡ºæ¯”'].mean()
        avg_ratio_10d = df['10æ—¥æµå…¥æµå‡ºæ¯”'].mean()
        
        # å‡€æµå…¥å‰10ï¼ˆå¤šæ—¶é—´ç»´åº¦ï¼‰
        top_inflow_3d = df.nlargest(10, '3æ—¥å‡€æµå…¥')[['è¡Œä¸šä»£ç ', 'è¡Œä¸šåç§°', '3æ—¥å‡€æµå…¥', '3æ—¥å¹³å‡æ¶¨è·Œå¹…']].to_dict('records')
        top_inflow_5d = df.nlargest(10, '5æ—¥å‡€æµå…¥')[['è¡Œä¸šä»£ç ', 'è¡Œä¸šåç§°', '5æ—¥å‡€æµå…¥', '5æ—¥å¹³å‡æ¶¨è·Œå¹…']].to_dict('records')
        top_inflow_10d = df.nlargest(10, '10æ—¥å‡€æµå…¥')[['è¡Œä¸šä»£ç ', 'è¡Œä¸šåç§°', '10æ—¥å‡€æµå…¥', '10æ—¥å¹³å‡æ¶¨è·Œå¹…']].to_dict('records')
        
        summary = {
            'total_industries': total_industries,
            'positive_3d_count': positive_3d,
            'positive_5d_count': positive_5d,
            'positive_10d_count': positive_10d,
            'total_inflow_3d_billion': total_inflow_3d / 1e8,
            'total_inflow_5d_billion': total_inflow_5d / 1e8,
            'total_inflow_10d_billion': total_inflow_10d / 1e8,
            'avg_ratio_3d': avg_ratio_3d,
            'avg_ratio_5d': avg_ratio_5d,
            'avg_ratio_10d': avg_ratio_10d,
            'top_inflow_3d': top_inflow_3d,
            'top_inflow_5d': top_inflow_5d,
            'top_inflow_10d': top_inflow_10d
        }
        
        return summary
    
    @staticmethod
    def analyze_market_structure(data):
        """
        å¯¹è¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘æ•°æ®è¿›è¡Œç»“æ„åŒ–åˆ†æ
        é‡ç‚¹åˆ†æå¤šæ—¶é—´ç»´åº¦çš„è¡Œä¸šè½®åŠ¨å’Œèµ„é‡‘è¶‹åŠ¿
        
        Args:
            data: è¡Œä¸šæ•°æ®åˆ—è¡¨
            
        Returns:
            dict: ç»“æ„åŒ–åˆ†æç»“æœ
        """
        if not data:
            return None
        
        df = pd.DataFrame(data)
        
        analysis_result = {
            'summary': {},
            'flow_trends': {},
            'sector_rotation': {},
            'risk_industries': {},
            'hot_industries': {}
        }
        
        # åŸºç¡€ç»Ÿè®¡
        total_industries = len(df)
        positive_3d = len(df[df['3æ—¥å¹³å‡æ¶¨è·Œå¹…'] > 0])
        positive_5d = len(df[df['5æ—¥å¹³å‡æ¶¨è·Œå¹…'] > 0])
        positive_10d = len(df[df['10æ—¥å¹³å‡æ¶¨è·Œå¹…'] > 0])
        
        # å‡€æµå…¥ç»Ÿè®¡
        total_inflow_3d = df['3æ—¥å‡€æµå…¥'].sum()
        total_inflow_5d = df['5æ—¥å‡€æµå…¥'].sum()
        total_inflow_10d = df['10æ—¥å‡€æµå…¥'].sum()
        
        # æµå…¥æµå‡ºæ¯”ç»Ÿè®¡
        avg_ratio_3d = df['3æ—¥æµå…¥æµå‡ºæ¯”'].mean()
        avg_ratio_5d = df['5æ—¥æµå…¥æµå‡ºæ¯”'].mean()
        avg_ratio_10d = df['10æ—¥æµå…¥æµå‡ºæ¯”'].mean()
        
        # åŸºç¡€åˆ†ææ±‡æ€»
        analysis_result['summary'] = {
            'total_industries': total_industries,
            'positive_3d_count': positive_3d,
            'positive_5d_count': positive_5d,
            'positive_10d_count': positive_10d,
            'total_inflow_3d_billion': total_inflow_3d / 1e8,
            'total_inflow_5d_billion': total_inflow_5d / 1e8,
            'total_inflow_10d_billion': total_inflow_10d / 1e8,
            'avg_ratio_3d': avg_ratio_3d,
            'avg_ratio_5d': avg_ratio_5d,
            'avg_ratio_10d': avg_ratio_10d
        }
        
        # èµ„é‡‘æµå‘è¶‹åŠ¿åˆ†æ
        big_inflow_3d = df[df['3æ—¥å‡€æµå…¥'] > 5e7]  # 3æ—¥å‡€æµå…¥è¶…5000ä¸‡
        big_inflow_5d = df[df['5æ—¥å‡€æµå…¥'] > 5e7]  # 5æ—¥å‡€æµå…¥è¶…5000ä¸‡
        big_inflow_10d = df[df['10æ—¥å‡€æµå…¥'] > 5e7]  # 10æ—¥å‡€æµå…¥è¶…5000ä¸‡
        
        analysis_result['flow_trends'] = {
            'big_inflow_3d_count': len(big_inflow_3d),
            'big_inflow_5d_count': len(big_inflow_5d),
            'big_inflow_10d_count': len(big_inflow_10d),
            'top_inflow_3d': big_inflow_3d.nlargest(10, '3æ—¥å‡€æµå…¥')[['è¡Œä¸šä»£ç ', 'è¡Œä¸šåç§°', '3æ—¥å‡€æµå…¥', '3æ—¥å¹³å‡æ¶¨è·Œå¹…']].to_dict('records'),
            'top_inflow_5d': big_inflow_5d.nlargest(10, '5æ—¥å‡€æµå…¥')[['è¡Œä¸šä»£ç ', 'è¡Œä¸šåç§°', '5æ—¥å‡€æµå…¥', '5æ—¥å¹³å‡æ¶¨è·Œå¹…']].to_dict('records'),
            'top_inflow_10d': big_inflow_10d.nlargest(10, '10æ—¥å‡€æµå…¥')[['è¡Œä¸šä»£ç ', 'è¡Œä¸šåç§°', '10æ—¥å‡€æµå…¥', '10æ—¥å¹³å‡æ¶¨è·Œå¹…']].to_dict('records')
        }
        
        # è¡Œä¸šè½®åŠ¨åˆ†æ
        # è®¡ç®—è¡Œä¸šå¼ºåº¦æŒ‡æ ‡
        df['3æ—¥å¼ºåº¦'] = df['3æ—¥å‡€æµå…¥'] * df['3æ—¥æµå…¥æµå‡ºæ¯”'] * (1 + df['3æ—¥å¹³å‡æ¶¨è·Œå¹…'])
        df['5æ—¥å¼ºåº¦'] = df['5æ—¥å‡€æµå…¥'] * df['5æ—¥æµå…¥æµå‡ºæ¯”'] * (1 + df['5æ—¥å¹³å‡æ¶¨è·Œå¹…'])
        df['10æ—¥å¼ºåº¦'] = df['10æ—¥å‡€æµå…¥'] * df['10æ—¥æµå…¥æµå‡ºæ¯”'] * (1 + df['10æ—¥å¹³å‡æ¶¨è·Œå¹…'])
        
        # çŸ­æœŸå¼ºåŠ¿è¡Œä¸šï¼ˆ3æ—¥ï¼‰
        strong_3d = df.nlargest(15, '3æ—¥å¼ºåº¦')[['è¡Œä¸šä»£ç ', 'è¡Œä¸šåç§°', '3æ—¥å¼ºåº¦', '3æ—¥å‡€æµå…¥', '3æ—¥æµå…¥æµå‡ºæ¯”', '3æ—¥å¹³å‡æ¶¨è·Œå¹…']].to_dict('records')
        
        # ä¸­æœŸå¼ºåŠ¿è¡Œä¸šï¼ˆ5æ—¥ï¼‰
        strong_5d = df.nlargest(15, '5æ—¥å¼ºåº¦')[['è¡Œä¸šä»£ç ', 'è¡Œä¸šåç§°', '5æ—¥å¼ºåº¦', '5æ—¥å‡€æµå…¥', '5æ—¥æµå…¥æµå‡ºæ¯”', '5æ—¥å¹³å‡æ¶¨è·Œå¹…']].to_dict('records')
        
        # é•¿æœŸå¼ºåŠ¿è¡Œä¸šï¼ˆ10æ—¥ï¼‰
        strong_10d = df.nlargest(15, '10æ—¥å¼ºåº¦')[['è¡Œä¸šä»£ç ', 'è¡Œä¸šåç§°', '10æ—¥å¼ºåº¦', '10æ—¥å‡€æµå…¥', '10æ—¥æµå…¥æµå‡ºæ¯”', '10æ—¥å¹³å‡æ¶¨è·Œå¹…']].to_dict('records')
        
        analysis_result['sector_rotation'] = {
            'strong_3d': strong_3d,
            'strong_5d': strong_5d,
            'strong_10d': strong_10d
        }
        
        # é£é™©è¡Œä¸šè¯†åˆ«
        # å¤§é¢æµå‡ºè¡Œä¸š
        big_outflow_3d = df[df['3æ—¥å‡€æµå…¥'] < -5e7]  # 3æ—¥å‡€æµå‡ºè¶…5000ä¸‡
        big_outflow_5d = df[df['5æ—¥å‡€æµå…¥'] < -5e7]  # 5æ—¥å‡€æµå‡ºè¶…5000ä¸‡
        big_outflow_10d = df[df['10æ—¥å‡€æµå…¥'] < -5e7]  # 10æ—¥å‡€æµå‡ºè¶…5000ä¸‡
        
        # è´Ÿæµå…¥æµå‡ºæ¯”è¡Œä¸šï¼ˆæµå‡ºå¤§äºæµå…¥ï¼‰
        negative_ratio_3d = df[df['3æ—¥æµå…¥æµå‡ºæ¯”'] < 0]
        negative_ratio_5d = df[df['5æ—¥æµå…¥æµå‡ºæ¯”'] < 0]
        negative_ratio_10d = df[df['10æ—¥æµå…¥æµå‡ºæ¯”'] < 0]
        
        analysis_result['risk_industries'] = {
            'big_outflow_3d_count': len(big_outflow_3d),
            'big_outflow_5d_count': len(big_outflow_5d),
            'big_outflow_10d_count': len(big_outflow_10d),
            'negative_ratio_3d_count': len(negative_ratio_3d),
            'negative_ratio_5d_count': len(negative_ratio_5d),
            'negative_ratio_10d_count': len(negative_ratio_10d),
            'big_outflow_3d': big_outflow_3d[['è¡Œä¸šä»£ç ', 'è¡Œä¸šåç§°', '3æ—¥å‡€æµå…¥', '3æ—¥æµå…¥æµå‡ºæ¯”']].to_dict('records'),
            'big_outflow_5d': big_outflow_5d[['è¡Œä¸šä»£ç ', 'è¡Œä¸šåç§°', '5æ—¥å‡€æµå…¥', '5æ—¥æµå…¥æµå‡ºæ¯”']].to_dict('records'),
            'big_outflow_10d': big_outflow_10d[['è¡Œä¸šä»£ç ', 'è¡Œä¸šåç§°', '10æ—¥å‡€æµå…¥', '10æ—¥æµå…¥æµå‡ºæ¯”']].to_dict('records')
        }
        
        # çƒ­é—¨è¡Œä¸šåˆ†æ
        # é«˜æµå…¥æµå‡ºæ¯”è¡Œä¸šï¼ˆèµ„é‡‘å…³æ³¨åº¦é«˜ï¼‰
        hot_ratio_3d = df[df['3æ—¥æµå…¥æµå‡ºæ¯”'] > 1.5]  # æµå…¥æ˜¯æµå‡ºçš„1.5å€ä»¥ä¸Š
        hot_ratio_5d = df[df['5æ—¥æµå…¥æµå‡ºæ¯”'] > 1.5]
        hot_ratio_10d = df[df['10æ—¥æµå…¥æµå‡ºæ¯”'] > 1.5]
        
        # å¤§é¢æµå…¥è¡Œä¸šï¼ˆèµ„é‡‘è§„æ¨¡å¤§ï¼‰
        hot_volume_3d = df[df['3æ—¥å‡€æµå…¥'] > 2e8]  # 3æ—¥å‡€æµå…¥è¶…2äº¿
        hot_volume_5d = df[df['5æ—¥å‡€æµå…¥'] > 2e8]  # 5æ—¥å‡€æµå…¥è¶…2äº¿
        hot_volume_10d = df[df['10æ—¥å‡€æµå…¥'] > 2e8]  # 10æ—¥å‡€æµå…¥è¶…2äº¿
        
        return analysis_result


# =============================================================================
# å¯è§†åŒ–æ¨¡å—
# =============================================================================

class DataVisualizer:
    """
    æ•°æ®å¯è§†åŒ–ç±» - ç”Ÿæˆè¡Œä¸šèµ„é‡‘æµå‘åˆ†æå›¾è¡¨
    """
    
    def __init__(self):
        """åˆå§‹åŒ–å¯è§†åŒ–é…ç½®"""
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['Noto Sans CJK SC', 'WenQuanYi Micro Hei', 'SimHei', 'Microsoft YaHei', 'PingFang SC', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # ç¡®ä¿å›¾ç‰‡ä¿å­˜ç›®å½•å­˜åœ¨
        if not os.path.exists(HUGO_IMAGES_DIR):
            os.makedirs(HUGO_IMAGES_DIR, exist_ok=True)
            
    def plot_top_inflow(self, data, days=3):
        """
        ç»˜åˆ¶å‡€æµå…¥å‰10è¡Œä¸šæŸ±çŠ¶å›¾
        
        Args:
            data: è¡Œä¸šæ•°æ®åˆ—è¡¨
            days: å¤©æ•°å‘¨æœŸ (3, 5, 10)
        """
        if not data:
            return None
            
        df = pd.DataFrame(data)
        col_name = f'{days}æ—¥å‡€æµå…¥'
        top_df = df.nlargest(10, col_name).sort_values(by=col_name, ascending=True)
        
        plt.figure(figsize=(10, 6))
        bars = plt.barh(top_df['è¡Œä¸šåç§°'], top_df[col_name] / 1e8, color='skyblue')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars:
            width = bar.get_width()
            plt.text(width, bar.get_y() + bar.get_height()/2, f' {width:.2f}äº¿', 
                     va='center', fontsize=10)
            
        plt.title(f'è¯ç›‘ä¼šè¡Œä¸š {days}æ—¥å‡€æµå…¥å‰10 (äº¿å…ƒ)', fontsize=14)
        plt.xlabel('å‡€æµå…¥é‡‘é¢ (äº¿å…ƒ)', fontsize=12)
        plt.grid(axis='x', linestyle='--', alpha=0.7)
        plt.tight_layout()
        
        filename = f"industry_inflow_{days}d.png"
        save_path = os.path.join(HUGO_IMAGES_DIR, filename)
        plt.savefig(save_path, dpi=120)
        plt.close()
        
        print(f"ğŸ“Š å›¾è¡¨å·²ä¿å­˜: {save_path}")
        return filename

    def plot_flow_scatter(self, data, days=5):
        """
        ç»˜åˆ¶èµ„é‡‘æµå‘å¯¹æ¯”æ•£ç‚¹å›¾ (å‡€æµå…¥å æ¯” vs æ¶¨è·Œå¹…)
        """
        if not data:
            return None
            
        df = pd.DataFrame(data)
        x_col = f'{days}æ—¥å‡€æµå…¥å æ¯”'
        y_col = f'{days}æ—¥å¹³å‡æ¶¨è·Œå¹…'
        
        plt.figure(figsize=(10, 8))
        plt.scatter(df[x_col], df[y_col] * 100, alpha=0.6, s=100, c='coral', edgecolors='white')
        
        # æ ‡è®°å‰5å¤§å‡€æµå…¥è¡Œä¸š
        top_5 = df.nlargest(5, f'{days}æ—¥å‡€æµå…¥')
        for i, row in top_5.iterrows():
            plt.annotate(row['è¡Œä¸šåç§°'], (row[x_col], row[y_col] * 100), 
                         xytext=(5, 5), textcoords='offset points', fontsize=9)
            
        plt.axhline(0, color='gray', linestyle='--', alpha=0.5)
        plt.axvline(0, color='gray', linestyle='--', alpha=0.5)
        
        plt.title(f'è¡Œä¸šèµ„é‡‘æµå‘åˆ†å¸ƒ ({days}æ—¥) - å æ¯” vs æ¶¨è·Œå¹…', fontsize=14)
        plt.xlabel('å‡€æµå…¥å æ¯” (%)', fontsize=12)
        plt.ylabel('å¹³å‡æ¶¨è·Œå¹… (%)', fontsize=12)
        plt.grid(True, linestyle=':', alpha=0.6)
        plt.tight_layout()
        
        filename = f"industry_scatter_{days}d.png"
        save_path = os.path.join(HUGO_IMAGES_DIR, filename)
        plt.savefig(save_path, dpi=120)
        plt.close()
        
        print(f"ğŸ“ˆ æ•£ç‚¹å›¾å·²ä¿å­˜: {save_path}")
        return filename

# =============================================================================
# AIåˆ†ææ¨¡å—
# =============================================================================

class AIAnalyzer:
    """
    AIåˆ†æç±» - ä¸“æ³¨äºè¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘AIæ™ºèƒ½åˆ†æ
    """
    
    @staticmethod
    def prepare_ai_context(data, analysis_result):
        """
        ä¸ºAIå‡†å¤‡ç»“æ„åŒ–çš„åˆ†æä¸Šä¸‹æ–‡
        é‡ç‚¹çªå‡ºè¯ç›‘ä¼šè¡Œä¸šå¤šæ—¶é—´ç»´åº¦åˆ†æ
        
        Args:
            data: åŸå§‹æ•°æ®
            analysis_result: ç»“æ„åŒ–åˆ†æç»“æœ
            
        Returns:
            str: AIåˆ†æçš„ä¸Šä¸‹æ–‡æ–‡æœ¬
        """
        if not analysis_result:
            return "æ•°æ®åˆ†æå¤±è´¥ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Šã€‚"
        
        summary = analysis_result['summary']
        flow_trends = analysis_result['flow_trends']
        sector_rotation = analysis_result['sector_rotation']
        risk_industries = analysis_result['risk_industries']
        hot_industries = analysis_result['hot_industries']
        
        # æ„å»ºCSVå†…å®¹ç”¨äºAIåˆ†æ
        df = pd.DataFrame(data)
        csv_content = df.to_csv(index=False, encoding='utf-8-sig')
        
        context = f"""
===========================================
ğŸš€ è¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘æ•°æ® - AIæ™ºèƒ½åˆ†æç‰ˆ
===========================================

ğŸ’° ã€å¸‚åœºæ•´ä½“æ¦‚å†µã€‘
- æ€»è¡Œä¸šæ•°: {summary['total_industries']}ä¸ª
- 3æ—¥ä¸Šæ¶¨è¡Œä¸š: {summary['positive_3d_count']}ä¸ª ({summary['positive_3d_count']/summary['total_industries']*100:.1f}%)
- 5æ—¥ä¸Šæ¶¨è¡Œä¸š: {summary['positive_5d_count']}ä¸ª ({summary['positive_5d_count']/summary['total_industries']*100:.1f}%)
- 10æ—¥ä¸Šæ¶¨è¡Œä¸š: {summary['positive_10d_count']}ä¸ª ({summary['positive_10d_count']/summary['total_industries']*100:.1f}%)
- 3æ—¥æ€»å‡€æµå…¥: {summary['total_inflow_3d_billion']:.2f}äº¿å…ƒ
- 5æ—¥æ€»å‡€æµå…¥: {summary['total_inflow_5d_billion']:.2f}äº¿å…ƒ
- 10æ—¥æ€»å‡€æµå…¥: {summary['total_inflow_10d_billion']:.2f}äº¿å…ƒ
- 3æ—¥å¹³å‡æµå…¥æµå‡ºæ¯”: {summary['avg_ratio_3d']:.3f}
- 5æ—¥å¹³å‡æµå…¥æµå‡ºæ¯”: {summary['avg_ratio_5d']:.3f}
- 10æ—¥å¹³å‡æµå…¥æµå‡ºæ¯”: {summary['avg_ratio_10d']:.3f}

ğŸ† ã€çŸ­æœŸå¼ºåŠ¿è¡Œä¸šTOP10ï¼ˆ3æ—¥ï¼‰ã€‘"""
        
        # çŸ­æœŸå¼ºåŠ¿è¡Œä¸šåˆ†æ
        for i, industry in enumerate(sector_rotation['strong_3d'][:10], 1):
            context += f"""
{i}. {industry['è¡Œä¸šåç§°']}({industry['è¡Œä¸šä»£ç ']})
   ğŸ’° å‡€æµå…¥: {industry['3æ—¥å‡€æµå…¥']/1e8:.2f}äº¿å…ƒ
   ğŸ“ˆ å¹³å‡æ¶¨å¹…: {industry['3æ—¥å¹³å‡æ¶¨è·Œå¹…']*100:.2f}%
   ğŸ”„ æµå…¥æµå‡ºæ¯”: {industry['3æ—¥æµå…¥æµå‡ºæ¯”']:.2f}
   â­ å¼ºåº¦æŒ‡æ•°: {industry['3æ—¥å¼ºåº¦']/1e8:.2f}"""
        
        context += f"""

ğŸ¯ ã€ä¸­æœŸå¼ºåŠ¿è¡Œä¸šTOP10ï¼ˆ5æ—¥ï¼‰ã€‘"""
        for i, industry in enumerate(sector_rotation['strong_5d'][:10], 1):
            context += f"""
{i}. {industry['è¡Œä¸šåç§°']}({industry['è¡Œä¸šä»£ç ']})
   ğŸ’° å‡€æµå…¥: {industry['5æ—¥å‡€æµå…¥']/1e8:.2f}äº¿å…ƒ
   ğŸ“ˆ å¹³å‡æ¶¨å¹…: {industry['5æ—¥å¹³å‡æ¶¨è·Œå¹…']*100:.2f}%
   ğŸ”„ æµå…¥æµå‡ºæ¯”: {industry['5æ—¥æµå…¥æµå‡ºæ¯”']:.2f}
   â­ å¼ºåº¦æŒ‡æ•°: {industry['5æ—¥å¼ºåº¦']/1e8:.2f}"""
        
        context += f"""

ğŸš€ ã€é•¿æœŸå¼ºåŠ¿è¡Œä¸šTOP10ï¼ˆ10æ—¥ï¼‰ã€‘"""
        for i, industry in enumerate(sector_rotation['strong_10d'][:10], 1):
            context += f"""
{i}. {industry['è¡Œä¸šåç§°']}({industry['è¡Œä¸šä»£ç ']})
   ğŸ’° å‡€æµå…¥: {industry['10æ—¥å‡€æµå…¥']/1e8:.2f}äº¿å…ƒ
   ğŸ“ˆ å¹³å‡æ¶¨å¹…: {industry['10æ—¥å¹³å‡æ¶¨è·Œå¹…']*100:.2f}%
   ğŸ”„ æµå…¥æµå‡ºæ¯”: {industry['10æ—¥æµå…¥æµå‡ºæ¯”']:.2f}
   â­ å¼ºåº¦æŒ‡æ•°: {industry['10æ—¥å¼ºåº¦']/1e8:.2f}"""
        
        context += f"""

ğŸ”¥ ã€å¤§é¢æµå…¥è¡Œä¸šæ±‡æ€»ã€‘
ğŸ“Š 3æ—¥å¤§é¢æµå…¥ï¼ˆè¶…5000ä¸‡ï¼‰: {flow_trends['big_inflow_3d_count']}ä¸ª"""
        
        for i, industry in enumerate(flow_trends['top_inflow_3d'][:5], 1):
            context += f"""
{i}. {industry['è¡Œä¸šåç§°']}({industry['è¡Œä¸šä»£ç ']}) - {industry['3æ—¥å‡€æµå…¥']/1e8:.2f}äº¿å…ƒ"""
        
        context += f"""
ğŸ“Š 5æ—¥å¤§é¢æµå…¥ï¼ˆè¶…5000ä¸‡ï¼‰: {flow_trends['big_inflow_5d_count']}ä¸ª"""
        for i, industry in enumerate(flow_trends['top_inflow_5d'][:5], 1):
            context += f"""
{i}. {industry['è¡Œä¸šåç§°']}({industry['è¡Œä¸šä»£ç ']}) - {industry['5æ—¥å‡€æµå…¥']/1e8:.2f}äº¿å…ƒ"""
        
        context += f"""
ğŸ“Š 10æ—¥å¤§é¢æµå…¥ï¼ˆè¶…5000ä¸‡ï¼‰: {flow_trends['big_inflow_10d_count']}ä¸ª"""
        for i, industry in enumerate(flow_trends['top_inflow_10d'][:5], 1):
            context += f"""
{i}. {industry['è¡Œä¸šåç§°']}({industry['è¡Œä¸šä»£ç ']}) - {industry['10æ—¥å‡€æµå…¥']/1e8:.2f}äº¿å…ƒ"""
        
        context += f"""

âš ï¸ ã€é£é™©è¡Œä¸šè­¦ç¤ºã€‘
ğŸ“‰ 3æ—¥å¤§é¢æµå‡ºï¼ˆè¶…5000ä¸‡ï¼‰: {risk_industries['big_outflow_3d_count']}ä¸ª"""
        if risk_industries['big_outflow_3d']:
            for i, industry in enumerate(risk_industries['big_outflow_3d'][:5], 1):
                context += f"""
{i}. {industry['è¡Œä¸šåç§°']}({industry['è¡Œä¸šä»£ç ']}) - å‡€æµå‡º: {abs(industry['3æ—¥å‡€æµå…¥'])/1e8:.2f}äº¿å…ƒ"""
        else:
            context += "\næ— å¤§é¢æµå‡ºè¡Œä¸š"
        
        context += f"""
ğŸ“‰ 5æ—¥å¤§é¢æµå‡ºï¼ˆè¶…5000ä¸‡ï¼‰: {risk_industries['big_outflow_5d_count']}ä¸ª"""
        if risk_industries['big_outflow_5d']:
            for i, industry in enumerate(risk_industries['big_outflow_5d'][:5], 1):
                context += f"""
{i}. {industry['è¡Œä¸šåç§°']}({industry['è¡Œä¸šä»£ç ']}) - å‡€æµå‡º: {abs(industry['5æ—¥å‡€æµå…¥'])/1e8:.2f}äº¿å…ƒ"""
        else:
            context += "\næ— å¤§é¢æµå‡ºè¡Œä¸š"
        
        context += f"""
ğŸ“‰ 10æ—¥å¤§é¢æµå‡ºï¼ˆè¶…5000ä¸‡ï¼‰: {risk_industries['big_outflow_10d_count']}ä¸ª"""
        if risk_industries['big_outflow_10d']:
            for i, industry in enumerate(risk_industries['big_outflow_10d'][:5], 1):
                context += f"""
{i}. {industry['è¡Œä¸šåç§°']}({industry['è¡Œä¸šä»£ç ']}) - å‡€æµå‡º: {abs(industry['10æ—¥å‡€æµå…¥'])/1e8:.2f}äº¿å…ƒ"""
        else:
            context += "\næ— å¤§é¢æµå‡ºè¡Œä¸š"
        
        context += f"""

ğŸŒŸ ã€è¡Œä¸šè½®åŠ¨æ´å¯Ÿã€‘
- çŸ­æœŸçƒ­ç‚¹: {sector_rotation['strong_3d'][0]['è¡Œä¸šåç§°'] if sector_rotation['strong_3d'] else 'æ— æ•°æ®'}
- ä¸­æœŸçƒ­ç‚¹: {sector_rotation['strong_5d'][0]['è¡Œä¸šåç§°'] if sector_rotation['strong_5d'] else 'æ— æ•°æ®'}
- é•¿æœŸçƒ­ç‚¹: {sector_rotation['strong_10d'][0]['è¡Œä¸šåç§°'] if sector_rotation['strong_10d'] else 'æ— æ•°æ®'}

===========================================
[å®Œæ•´CSVæ•°æ® - ç”¨äºæ·±åº¦åˆ†æ]
{csv_content}
===========================================
"""
        
        # ========== æ–°å¢ï¼šæ‰“å°AIä¸Šä¸‹æ–‡ä¿¡æ¯ ==========
        print("ğŸ“Š AIåˆ†æä¸Šä¸‹æ–‡æ„å»ºå®Œæˆ:")
        print(f"   - ä¸Šä¸‹æ–‡æ€»å­—ç¬¦æ•°: {len(context):,} å­—ç¬¦")
        print(f"   - ä¸Šä¸‹æ–‡è¡Œæ•°: {len(context.split(chr(10)))} è¡Œ")
        print(f"   - è¡Œä¸šæ€»æ•°: {summary['total_industries']} ä¸ª")
        print(f"   - æ•°æ®è¦†ç›–: 3æ—¥/5æ—¥/10æ—¥ å¤šæ—¶é—´ç»´åº¦")
        print("="*50)
        print("ğŸ“‹ ä¸Šä¸‹æ–‡å†…å®¹é¢„è§ˆ:")
        print("-" * 50)
        print(context[:500] + "..." if len(context) > 500 else context)
        print("-" * 50)
        print("="*50)
        
        return context
    
    @staticmethod
    def call_ai_analysis(data, analysis_result):
        """
        è°ƒç”¨DeepSeekè¿›è¡Œè¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘AIåˆ†æ
        
        Args:
            data: åŸå§‹æ•°æ®
            analysis_result: ç»“æ„åŒ–åˆ†æç»“æœ
            
        Returns:
            str: AIåˆ†ææŠ¥å‘Š
        """
        if not DEEPSEEK_API_KEY or "sk-" not in DEEPSEEK_API_KEY:
            print("[Warning] æœªé…ç½® DEEPSEEK_API_KEYï¼Œè·³è¿‡ AI åˆ†æã€‚")
            return "æœªé…ç½® API Keyï¼Œæ— æ³•ç”Ÿæˆ AI æŠ¥å‘Šã€‚"
        
        # å‡†å¤‡AIä¸Šä¸‹æ–‡
        context = AIAnalyzer.prepare_ai_context(data, analysis_result)
        
        # æ„å»ºä¸“ä¸šæç¤ºè¯
        prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“æ³¨äºä¸­å›½Aè‚¡å¸‚åœºè¯ç›‘ä¼šè¡Œä¸šåˆ†ç±»èµ„é‡‘æµå‘åˆ†æçš„èµ„æ·±é‡‘èåˆ†æå¸ˆã€‚
æˆ‘å°†ä¸ºä½ æä¾›æœ€æ–°çš„è¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘æ•°æ®ï¼ŒåŒ…å«3æ—¥ã€5æ—¥ã€10æ—¥å¤šæ—¶é—´ç»´åº¦çš„å®Œæ•´åˆ†æã€‚

{context}

è¯·æ ¹æ®æä¾›çš„æ•°æ®ç”Ÿæˆä¸€ä»½å…¨é¢çš„è¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘AIåˆ†ææŠ¥å‘Šï¼Œæ¶µç›–ä»¥ä¸‹æ–¹é¢ï¼š
ğŸ’° **èµ„é‡‘æµå‘è¶‹åŠ¿åˆ†æ**ï¼šåˆ†æ3æ—¥ã€5æ—¥ã€10æ—¥å‡€æµå…¥/å‡€æµå‡ºæƒ…å†µï¼Œåˆ¤æ–­çŸ­æœŸã€ä¸­æœŸã€é•¿æœŸèµ„é‡‘è¶‹åŠ¿  
ğŸ”¥ **è¡Œä¸šè½®åŠ¨åˆ†æ**ï¼šè¯†åˆ«çŸ­æœŸã€ä¸­æœŸã€é•¿æœŸå¼ºåŠ¿è¡Œä¸šï¼Œåˆ†æè¡Œä¸šè½®åŠ¨è§„å¾‹  
ğŸ“ˆ **è¡Œä¸šå¼ºåº¦æ’å**ï¼šåŸºäºå¤šæ—¶é—´ç»´åº¦ç»¼åˆè¯„åˆ†ï¼Œè¯†åˆ«æœ€å¼ºè¡Œä¸šå’Œæœ€å¼±è¡Œä¸š  
âš ï¸ **é£é™©è¡Œä¸šè­¦ç¤º**ï¼šå¯¹äºå¤§é¢æµå‡ºã€è´Ÿæµå…¥æµå‡ºæ¯”çš„è¡Œä¸šç»™å‡ºé£é™©æç¤º  
ğŸ¯ **æŠ•èµ„ç­–ç•¥å»ºè®®**ï¼šåŸºäºè¡Œä¸šèµ„é‡‘æµå‘åˆ†æï¼Œç»™å‡ºçŸ­çº¿ã€ä¸­é•¿çº¿çš„è¡Œä¸šæŠ•èµ„å»ºè®®  
ğŸ“Š **å¸‚åœºæƒ…ç»ªåˆ¤æ–­**ï¼šé€šè¿‡è¡Œä¸šèµ„é‡‘æµå‘æ•°æ®åˆ¤æ–­å½“å‰å¸‚åœºæ•´ä½“æƒ…ç»ªå’Œè¡Œä¸šåå¥½  
ğŸŒŠ **è¡Œä¸šè½®åŠ¨æ´å¯Ÿ**ï¼šåˆ†æè¡Œä¸šè½®åŠ¨çš„çŸ­æœŸã€ä¸­æœŸã€é•¿æœŸé€»è¾‘

**é‡è¦è¦æ±‚**ï¼š
- è¯·ç›´æ¥è¾“å‡ºä¸­æ–‡æŠ¥å‘Šï¼Œç¡®ä¿åˆ†ææ·±å…¥ä¸”å…·æœ‰å®é™…æŒ‡å¯¼ä»·å€¼
- **ä¸è¦ä½¿ç”¨è¡¨æ ¼æ ¼å¼**ï¼Œä¿æŒæŠ¥å‘Šçš„ç®€æ´æ€§å’Œæ˜“è¯»æ€§
- **æŠ¥å‘Šå†…å®¹è¯·ä¸¥æ ¼æ§åˆ¶åœ¨3500å­—ä»¥å†…**
- é‡ç‚¹å…³æ³¨å¤šæ—¶é—´ç»´åº¦çš„å¯¹æ¯”åˆ†æå’Œè¡Œä¸šè½®åŠ¨è§„å¾‹
- ä½¿ç”¨Markdownæ ¼å¼ï¼Œç»“æ„æ¸…æ™°ï¼Œä¾¿äºé˜…è¯»
"""

        # ========== æ–°å¢ï¼šæ‰“å°å®Œæ•´æç¤ºè¯åŠŸèƒ½ ==========
        print("\n" + "="*80)
        print("ğŸš€ å‡†å¤‡å‘é€ç»™ DeepSeek API çš„å®Œæ•´æç¤ºè¯:")
        print("="*80)
        print("\nğŸ“‹ ã€ç³»ç»Ÿæç¤ºè¯ã€‘:")
        print("ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„Aè‚¡è¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘åˆ†æå¸ˆã€‚")
        print("\nğŸ“ ã€ç”¨æˆ·æç¤ºè¯ã€‘:")
        print(prompt)
        print("\n" + "="*80)
        print(f"ğŸ“Š ã€æç¤ºè¯ç»Ÿè®¡ä¿¡æ¯ã€‘:")
        print(f"   - æ€»å­—ç¬¦æ•°: {len(prompt):,} å­—ç¬¦")
        print(f"   - è¡Œæ•°: {len(prompt.split(chr(10)))} è¡Œ")
        print(f"   - é¢„è®¡ä»¤ç‰Œæ•°: ~{len(prompt.split()) * 1.3:.0f} tokens")
        print("="*80)
        
        print("\n=== å‘ DeepSeek API å‘é€è¯·æ±‚ ===")
        
        # ä½¿ç”¨requestsç›´æ¥è°ƒç”¨DeepSeek API
        headers = {
            "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„Aè‚¡è¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘åˆ†æå¸ˆã€‚"},
                {"role": "user", "content": prompt}
            ],
            "stream": False
        }

        try:
            response = requests.post(
                f"{DEEPSEEK_BASE_URL}/chat/completions",
                headers=headers,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    report = result['choices'][0]['message']['content']
                    return report
                else:
                    print(f"DeepSeek API è¿”å›æ ¼å¼å¼‚å¸¸: {result}")
                    return "DeepSeek API è¿”å›æ ¼å¼å¼‚å¸¸"
            else:
                print(f"DeepSeek API è¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
                return f"DeepSeek API è¯·æ±‚å¤±è´¥: {response.status_code}"
                
        except Exception as e:
            print(f"è°ƒç”¨ DeepSeek API å‡ºé”™: {e}")
            return f"AI åˆ†æå¤±è´¥: {e}"

# =============================================================================
# æŠ¥å‘Šç”Ÿæˆæ¨¡å—
# =============================================================================

class ReportGenerator:
    """
    æŠ¥å‘Šç”Ÿæˆç±» - ä¸“æ³¨äºè¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘åˆ†ææŠ¥å‘Šç”Ÿæˆ
    """
    
    @staticmethod
    def generate_analysis_report(data, ai_report, image_filenames=None, filename=None):
        """
        ç”Ÿæˆè¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘AIåˆ†ææŠ¥å‘Šæ–‡ä»¶
        
        Args:
            data: åŸå§‹æ•°æ®
            ai_report: AIåˆ†ææŠ¥å‘Š
            image_filenames: åˆ—è¡¨ï¼ŒåŒ…å«ç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶å
            filename: æ–‡ä»¶åï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            str: ä¿å­˜çš„æ–‡ä»¶åï¼Œå¤±è´¥è¿”å›None
        """
        if filename is None:
            filename = "AIè¡Œä¸šèµ„é‡‘æµå‘åˆ†ææŠ¥å‘Š.md"
        
        try:
            # è·å–å¸‚åœºæ¦‚å†µ
            summary = DataAnalyzer.get_industry_summary(data)
            beijing_now = get_beijing_time()
            date_iso = beijing_now.strftime('%Y-%m-%dT%H:%M:%S+08:00')
            
            # ç»Ÿä¸€å›ºå®šæ ‡é¢˜
            fixed_title = "ğŸ“ˆAIè¡Œä¸šèµ„é‡‘æµå‘åˆ†ææŠ¥å‘Š"
            
            # æ„å»º Hugo åšå®¢æ ¼å¼çš„å†…å®¹ (Front Matter)
            front_matter = f"""---
title: "{fixed_title}"
date: {date_iso}
lastmod: {date_iso}
description: "åŸºäºæ–°æµªè´¢ç»è¯ç›‘ä¼šè¡Œä¸šæ•°æ®çš„AIæ·±åº¦èµ„é‡‘æµå‘åˆ†ææŠ¥å‘Šï¼Œæ¶µç›–3æ—¥ã€5æ—¥åŠ10æ—¥å¤šç»´åº¦è¶‹åŠ¿ã€‚"
draft: false
categories: ["è¡Œä¸šåˆ†æ"]
tags: ["Aè‚¡", "èµ„é‡‘æµå‘", "AIåˆ†æ", "è¯ç›‘ä¼šè¡Œä¸š"]
author: ["AIåˆ†æå¸ˆ"]
---
"""
            
            # æ’å…¥å¯è§†åŒ–å›¾è¡¨
            image_section = "## ğŸ“ˆ è¡Œä¸šèµ„é‡‘æµå‘å¯è§†åŒ–\n\n"
            if image_filenames:
                for img_file in image_filenames:
                    title = "è¡Œä¸šæ•°æ®å›¾è¡¨"
                    if "inflow" in img_file:
                        days = img_file.split("_")[-1].replace("d.png", "")
                        title = f"{days}æ—¥å‡€æµå…¥å‰10è¡Œä¸š"
                    elif "scatter" in img_file:
                        days = img_file.split("_")[-1].replace("d.png", "")
                        title = f"{days}æ—¥èµ„é‡‘æµå‘åˆ†å¸ƒæ•£ç‚¹å›¾"
                    
                    image_section += f"### {title}\n![{title}](/images/charts/{img_file})\n\n"
            else:
                image_section = ""
            
            # ç”ŸæˆæŠ¥å‘Šæ­£æ–‡
            report_body = f"""
## ğŸ“Š è¡Œä¸šæ•´ä½“æ¦‚å†µ
- **æ•°æ®è·å–æ—¶é—´**: {beijing_now.strftime('%Y-%m-%d %H:%M:%S')}
- **è¡Œä¸šæ€»æ•°**: {summary['total_industries']}ä¸ª
- **3æ—¥ä¸Šæ¶¨è¡Œä¸š**: {summary['positive_3d_count']}ä¸ª ({summary['positive_3d_count']/summary['total_industries']*100:.1f}%)
- **5æ—¥ä¸Šæ¶¨è¡Œä¸š**: {summary['positive_5d_count']}ä¸ª ({summary['positive_5d_count']/summary['total_industries']*100:.1f}%)
- **10æ—¥ä¸Šæ¶¨è¡Œä¸š**: {summary['positive_10d_count']}ä¸ª ({summary['positive_10d_count']/summary['total_industries']*100:.1f}%)
- **3æ—¥æ€»å‡€æµå…¥**: {summary['total_inflow_3d_billion']:.2f}äº¿å…ƒ
- **5æ—¥æ€»å‡€æµå…¥**: {summary['total_inflow_5d_billion']:.2f}äº¿å…ƒ
- **10æ—¥æ€»å‡€æµå…¥**: {summary['total_inflow_10d_billion']:.2f}äº¿å…ƒ
- **3æ—¥å¹³å‡æµå…¥æµå‡ºæ¯”**: {summary['avg_ratio_3d']:.3f}
- **5æ—¥å¹³å‡æµå…¥æµå‡ºæ¯”**: {summary['avg_ratio_5d']:.3f}
- **10æ—¥å¹³å‡æµå…¥æµå‡ºæ¯”**: {summary['avg_ratio_10d']:.3f}

## ğŸ’° å‡€æµå…¥å‰10è¡Œä¸šï¼ˆ3æ—¥ï¼‰
"""
            for i, stock in enumerate(summary['top_inflow_3d'], 1):
                report_body += f"{i}. **{stock['è¡Œä¸šåç§°']}**({stock['è¡Œä¸šä»£ç ']}) - å‡€æµå…¥: {stock['3æ—¥å‡€æµå…¥']/1e8:.2f}äº¿å…ƒ, æ¶¨è·Œå¹…: {stock['3æ—¥å¹³å‡æ¶¨è·Œå¹…']*100:.2f}%\n"
            
            report_body += f"""
## ğŸ’° å‡€æµå…¥å‰10è¡Œä¸šï¼ˆ5æ—¥ï¼‰
"""
            for i, stock in enumerate(summary['top_inflow_5d'], 1):
                report_body += f"{i}. **{stock['è¡Œä¸šåç§°']}**({stock['è¡Œä¸šä»£ç ']}) - å‡€æµå…¥: {stock['5æ—¥å‡€æµå…¥']/1e8:.2f}äº¿å…ƒ, æ¶¨è·Œå¹…: {stock['5æ—¥å¹³å‡æ¶¨è·Œå¹…']*100:.2f}%\n"
            
            report_body += f"""
## ğŸ’° å‡€æµå…¥å‰10è¡Œä¸šï¼ˆ10æ—¥ï¼‰
"""
            for i, stock in enumerate(summary['top_inflow_10d'], 1):
                report_body += f"{i}. **{stock['è¡Œä¸šåç§°']}**({stock['è¡Œä¸šä»£ç ']}) - å‡€æµå…¥: {stock['10æ—¥å‡€æµå…¥']/1e8:.2f}äº¿å…ƒ, æ¶¨è·Œå¹…: {stock['10æ—¥å¹³å‡æ¶¨è·Œå¹…']*100:.2f}%\n"
            
            report_body += f"""
## ğŸ¤– AIæ™ºèƒ½åˆ†ææŠ¥å‘Š

{ai_report}

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {beijing_now.strftime('%Y-%m-%d %H:%M:%S')}*
*æ•°æ®æ¥æº: æ–°æµªè´¢ç»è¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘æ•°æ®*
*åˆ†æå·¥å…·: DeepSeek AI*
"""
            
            # ç»„åˆå®Œæ•´å†…å®¹
            report_content = front_matter + image_section + report_body
            
            # ä¿å­˜æŠ¥å‘Š
            filepath = os.path.join(HUGO_CONTENT_DIR, filename)
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            print(f"AIåˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {filepath}")
            return filepath
            
        except Exception as e:
            print(f"ç”Ÿæˆåˆ†ææŠ¥å‘Šå‡ºé”™: {str(e)}")
            return None

# =============================================================================
# ä¸»ç¨‹åºæ¨¡å—
# =============================================================================

class CSRCIndustryAIAnalyzer:
    """
    è¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘AIåˆ†æå¸ˆ - ä¸»æ§åˆ¶å™¨
    æ•´åˆæ•°æ®è·å–ã€åˆ†æã€AIå¤„ç†å’Œæ¨é€åŠŸèƒ½
    """
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.data_fetcher = DataFetcher()
        self.data_analyzer = DataAnalyzer()
        self.ai_analyzer = AIAnalyzer()
        self.visualizer = DataVisualizer()
        self.report_generator = ReportGenerator()
    
    def run_analysis(self, total_pages=8, page_size=20):
        """
        è¿è¡Œå®Œæ•´çš„è¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘åˆ†ææµç¨‹
        
        Args:
            total_pages: è·å–çš„æ•°æ®é¡µæ•°
            page_size: æ¯é¡µæ•°æ®é‡
            
        Returns:
            dict: åˆ†æç»“æœï¼ŒåŒ…å«æ–‡ä»¶è·¯å¾„ç­‰ä¿¡æ¯
        """
        results = {
            'data': None,
            'csv_file': None,
            'summary': None,
            'analysis_result': None,
            'ai_report': None,
            'report_file': None
        }
        
        print("=== è¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘AIåˆ†æå¸ˆ ===")
        
        # 1. è·å–æ•°æ®ï¼ˆè·å–å®Œæ•´æ•°æ®ï¼‰
        print("\n=== ç¬¬ä¸€æ­¥ï¼šè·å–è¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘æ•°æ® ===")
        results['data'] = self.data_fetcher.collect_batch_data(total_pages=total_pages, page_size=page_size)
        
        if not results['data']:
            print("âŒ è¡Œä¸šæ•°æ®è·å–å¤±è´¥ï¼Œæ— æ³•ç»§ç»­åˆ†æ")
            return results
        
        # 2. åŸºç¡€è¡Œä¸šåˆ†æ
        print("\n=== ç¬¬äºŒæ­¥ï¼šåŸºç¡€è¡Œä¸šå¸‚åœºåˆ†æ ===")
        results['summary'] = self.data_analyzer.get_industry_summary(results['data'])
        self._print_summary(results['summary'])
        
        # 3. ç»“æ„åŒ–åˆ†æ
        print("\n=== ç¬¬ä¸‰æ­¥ï¼šæ·±åº¦è¡Œä¸šç»“æ„åŒ–åˆ†æ ===")
        results['analysis_result'] = self.data_analyzer.analyze_market_structure(results['data'])
        
        # 4. AIæ™ºèƒ½åˆ†æ
        print("\n=== ç¬¬å››æ­¥ï¼šAIæ™ºèƒ½è¡Œä¸šåˆ†æ ===")
        results['ai_report'] = self.ai_analyzer.call_ai_analysis(results['data'], results['analysis_result'])
        
        if results['ai_report'] and not results['ai_report'].startswith("æœªé…ç½®"):
            print("âœ… AIè¡Œä¸šåˆ†æå®Œæˆ")
            print("AIè¡Œä¸šåˆ†æç»“æœ:")
            print("-" * 50)
            print(results['ai_report'])
            print("-" * 50)
            
            print(f"\nğŸ‰ è¡Œä¸šåˆ†æå®Œæˆï¼")
            print(f"ğŸ“„ AIæŠ¥å‘Š: DeepSeekè¿”å›çš„markdownæ ¼å¼åˆ†ææŠ¥å‘Š")
            
            # 5. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
            print("\n=== ç¬¬äº”æ­¥ï¼šç”Ÿæˆè¡Œä¸šå¯è§†åŒ–å›¾è¡¨ ===")
            image_filenames = []
            try:
                # ç”Ÿæˆ3æ—¥ã€5æ—¥ã€10æ—¥çš„æµå…¥å›¾
                for d in [3, 5, 10]:
                    fn = self.visualizer.plot_top_inflow(results['data'], days=d)
                    if fn: image_filenames.append(fn)
                
                # ç”Ÿæˆ5æ—¥æ•£ç‚¹å›¾
                fn_s = self.visualizer.plot_flow_scatter(results['data'], days=5)
                if fn_s: image_filenames.append(fn_s)
            except Exception as e:
                print(f"âš ï¸ å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")

            # 6. ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
            print("\n=== ç¬¬å…­æ­¥ï¼šç”Ÿæˆåˆ†ææŠ¥å‘Šæ–‡ä»¶ ===")
            results['report_file'] = self.report_generator.generate_analysis_report(
                results['data'], results['ai_report'], image_filenames=image_filenames)
        else:
            print("âŒ AIè¡Œä¸šåˆ†æå¤±è´¥æˆ–æœªé…ç½®API")
        
        return results
    
    def _print_summary(self, summary):
        """æ‰“å°åˆ†ææ‘˜è¦"""
        print("è¡Œä¸šå¸‚åœºæ¦‚å†µ:")
        for key, value in summary.items():
            if not key.startswith('top_inflow'):
                print(f"  {key}: {value}")
        
        print("\nå‡€æµå…¥å‰5è¡Œä¸šï¼ˆ3æ—¥ï¼‰:")
        for i, stock in enumerate(summary['top_inflow_3d'][:5], 1):
            print(f"  {i}. {stock['è¡Œä¸šåç§°']}({stock['è¡Œä¸šä»£ç ']}) - {stock['3æ—¥å‡€æµå…¥']/1e8:.2f}äº¿å…ƒ")

def main():
    """ä¸»å‡½æ•° - è¯ç›‘ä¼šè¡Œä¸šèµ„é‡‘æµå‘æ•°æ®è·å–ä¸AIåˆ†æ"""
    analyzer = CSRCIndustryAIAnalyzer()
    
    # è¿è¡Œå®Œæ•´åˆ†ææµç¨‹
    results = analyzer.run_analysis(
        total_pages=8,     # è·å–8é¡µå®Œæ•´æ•°æ®
        page_size=20       # æ¯é¡µ20æ¡æ•°æ®
    )
    
    return results

if __name__ == "__main__":
    main()
